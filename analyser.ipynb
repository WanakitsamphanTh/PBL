{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "corpus_n = 3\n",
    "for i in range(corpus_n):\n",
    "    corpus.append(pd.read_csv('./corpus_' + str(i+1) + '.csv',delimiter=',',encoding='utf-8').dropna())\n",
    "    corpus[i].columns = ['question','item_in_question','question_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionsDataset(Dataset):\n",
    "    def __init__(self, file_paths, tokenizer, max_len):\n",
    "        #self.data = pd.read_csv(file_path)\n",
    "        self.data = pd.DataFrame(columns = ['question','item_in_question','question_type'])\n",
    "        for f in file_paths:\n",
    "            c = pd.read_csv(f,delimiter=',',encoding='utf-8').dropna()\n",
    "            self.data = pd.concat([self.data, c], ignore_index=True, sort=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question = self.data.loc[index, 'question']\n",
    "        item_in_question = self.data.loc[index, 'item_in_question']\n",
    "        question_type = self.data.loc[index, 'question_type']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "        targets = {\n",
    "            'item_in_question': item_in_question,\n",
    "            'question_type': question_type\n",
    "        }\n",
    "\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize dataset\n",
    "dataset = QuestionsDataset(['./corpus_1.csv','./corpus_2.csv'],tokenizer,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword Extraction\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertForKeywordExtraction(nn.Module):\n",
    "    def __init__(self, model_name, num_labels_item, num_labels_type):\n",
    "        super(BertForKeywordExtraction, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.classifier_item = nn.Linear(self.bert.config.hidden_size, num_labels_item)\n",
    "        self.classifier_type = nn.Linear(self.bert.config.hidden_size, num_labels_type)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        item_logits = self.classifier_item(pooled_output)\n",
    "        type_logits = self.classifier_type(pooled_output)\n",
    "        return item_logits, type_logits\n",
    "\n",
    "# Initialize the model\n",
    "data = dataset.data\n",
    "num_labels_item = len(set(data['item_in_question']))\n",
    "num_labels_type = len(set(data['question_type']))\n",
    "model = BertForKeywordExtraction('cl-tohoku/bert-base-japanese', num_labels_item, num_labels_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 3.7039\n",
      "Epoch 2/3, Loss: 2.6531\n",
      "Epoch 3/3, Loss: 2.1228\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_model(model, dataset, tokenizer, num_epochs=3, batch_size=16, learning_rate=3e-5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[0]['input_ids'].to(device)\n",
    "            attention_mask = batch[0]['attention_mask'].to(device)\n",
    "            item_in_question = batch[1]['item_in_question'].to(device)\n",
    "            question_type = batch[1]['question_type'].to(device)\n",
    "\n",
    "            item_logits, type_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_item = loss_fn(item_logits, item_in_question)\n",
    "            loss_type = loss_fn(type_logits, question_type)\n",
    "            loss = loss_item + loss_type\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "# Prepare the dataset and labels\n",
    "item_to_idx = {item: idx for idx, item in enumerate(data['item_in_question'].unique())}\n",
    "type_to_idx = {type_: idx for idx, type_ in enumerate(data['question_type'].unique())}\n",
    "\n",
    "data['item_in_question'] = data['item_in_question'].map(item_to_idx)\n",
    "data['question_type'] = data['question_type'].map(type_to_idx)\n",
    "\n",
    "# Save the processed dataset\n",
    "data.to_csv('questions_processed.csv', index=False)\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = QuestionsDataset(['questions_processed.csv'], tokenizer, max_len=128)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataset, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ユニバーサルグリル': 0, 'ガラリ': 1, '吸込口': 2, 'シーリングディフューザー': 3, '吹出口': 4, 'スリットグリル': 5, 'ノズル': 6, 'ラインディフューザー': 7, 'エアフィルター': 8, '防火ダンパー': 9, '一般ダンパー': 10, '排煙口': 11, 'フィルターケーシング': 12, 'ダンパー': 13}\n"
     ]
    }
   ],
   "source": [
    "print(item_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: ラインディフューザー, Question Type: いくら\n"
     ]
    }
   ],
   "source": [
    "idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "idx_to_type = {idx: type_ for type_, idx in type_to_idx.items()}\n",
    "\n",
    "def predict_keywords(question, model, tokenizer):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        question,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        item_logits, type_logits = model(input_ids, attention_mask)\n",
    "\n",
    "    item_idx = torch.argmax(item_logits, dim=1).item()\n",
    "    type_idx = torch.argmax(type_logits, dim=1).item()\n",
    "\n",
    "    return idx_to_item[item_idx], idx_to_type[type_idx]\n",
    "\n",
    "# Example prediction\n",
    "question = \"ラインディフューザーの料金はいくらですか\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.8160\n",
      "Epoch 2/3, Loss: 1.2692\n",
      "Epoch 3/3, Loss: 1.0489\n"
     ]
    }
   ],
   "source": [
    "train_model(model, dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForKeywordExtraction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (classifier_item): Linear(in_features=768, out_features=14, bias=True)\n",
       "  (classifier_type): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9055\n",
      "Epoch 2/3, Loss: 0.6370\n",
      "Epoch 3/3, Loss: 0.5256\n"
     ]
    }
   ],
   "source": [
    "train_model(model, dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model-3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'item_to_idx':item_to_idx,'type_to_idx':type_to_idx,'idx_to_item':idx_to_item,'idx_to_type':idx_to_type}\n",
    "np.save('converter.npy', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id, type_id = predict_keywords(\"グリルの長さを教えてください\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: ラインディフューザー, Question Type: いくら\n"
     ]
    }
   ],
   "source": [
    "question = \"ラインディフューザーの料金はいくらですか\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: ユニバーサルグリル, Question Type: 長さ\n"
     ]
    }
   ],
   "source": [
    "question = \"グリルの長さを教えてください\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: スリットグリル, Question Type: いつ\n"
     ]
    }
   ],
   "source": [
    "question = \"スリットグリルの納品について知りたいです。\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: スリットグリル, Question Type: 長さ\n"
     ]
    }
   ],
   "source": [
    "question = \"スリットグリルの長さはどのぐらいでしょうか。\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: スリットグリル, Question Type: いくら\n"
     ]
    }
   ],
   "source": [
    "question = \"スリットグリルの価格を教えてください\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: ノズル, Question Type: 長さ\n"
     ]
    }
   ],
   "source": [
    "question = \"グリルは何センチ長いですか\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: 一般ダンパー, Question Type: いくら\n"
     ]
    }
   ],
   "source": [
    "question = \"一般ダンパーの費用はどのぐらいですか。教えてください。\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item in Question: 一般ダンパー, Question Type: 重さ\n"
     ]
    }
   ],
   "source": [
    "question = \"一般ダンパーは何グラムですか\"\n",
    "item, qtype = predict_keywords(question, model, tokenizer)\n",
    "print(f\"Item in Question: {item}, Question Type: {qtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
